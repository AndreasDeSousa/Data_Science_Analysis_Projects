{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymeK3K4P879t"
   },
   "source": [
    "<h1 style=\"color:Orange;font-size:170%;\">New York City Taxi Fare Prediction</h>\n",
    "\n",
    "\n",
    "\n",
    "![](Taxi.jpg)\n",
    "\n",
    "Dataset Link: [https://www.kaggle.com/c/new-york-city-taxi-fare-prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction)\n",
    "\n",
    "We'll train a machine learning model to predict the fare for a taxi ride in New York city given information like pickup date & time, pickup location, drop location and no. of passengers. \n",
    "\n",
    "This dataset is taken from a [Kaggle competition](ttps://www.kaggle.com/c/new-york-city-taxi-fare-prediction) organized by Google Cloud.\n",
    "It contains over 55 millions rows of training data. We'll attempt to achieve a respectable score in the competition using just a fraction of the data.\n",
    "Along the way, we'll also look at some practical tips for machine learning. PMost of the ideas & techniques covered in this notebook are derived from\n",
    "other public notebooks & blog posts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovhvtVIlVM0w"
   },
   "source": [
    "\n",
    "> _**TIP #1**: Create an outline for your notebook & for each section before you start coding_\n",
    "\n",
    "\n",
    "\n",
    "Here's an outline of the project:\n",
    "\n",
    "1. Download the dataset\n",
    "2. Explore & analyze the dataset\n",
    "3. Prepare the dataset for ML training\n",
    "4. Train hardcoded & baseline models\n",
    "5. Make predictions & submit to Kaggle\n",
    "6. Perform feature engineering\n",
    "7. Train & evaluate different models\n",
    "8. Tune hyper-parameters for the best models\n",
    "9. Train on a GPU with the entire dataset\n",
    "10. Document & publish the project online\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KprVTPb44pV7"
   },
   "source": [
    "## 1. Download the Dataset\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Install required libraries\n",
    "- Download data from Kaggle\n",
    "- View dataset files\n",
    "- Load training set with Pandas\n",
    "- Load test set with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoD0siP-6b5K"
   },
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BMPQKKyo6hR_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in c:\\users\\desou\\anaconda3\\lib\\site-packages (0.1.20)\n",
      "Requirement already satisfied: kaggle in c:\\users\\desou\\anaconda3\\lib\\site-packages (from opendatasets) (1.5.12)\n",
      "Requirement already satisfied: tqdm in c:\\users\\desou\\anaconda3\\lib\\site-packages (from opendatasets) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\desou\\anaconda3\\lib\\site-packages (from opendatasets) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\desou\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\desou\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.25.1)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\desou\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\desou\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2020.12.5)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\desou\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\desou\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.26.4)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\desou\\anaconda3\\lib\\site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\desou\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\desou\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.0-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\desou\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\desou\\anaconda3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets --upgrade\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install scikit-learn\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "7RwbGaaTLJok"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(60000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR7X9VVl6h1J"
   },
   "source": [
    "### Download Data from Kaggle\n",
    "\n",
    "We'll use the [opendatasets](https://github.com/JovianML/opendatasets) library to download the dataset. You'll need to download your Kaggle API key that is a file called `kaggle.json` from Kaggle. Log-in to Kaggle and go to *Profile picture -> Account -> API -> Create New API* and download it on the same folder as the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RS0BaBV86j0Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.00/1.56G [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading new-york-city-taxi-fare-prediction.zip to .\\new-york-city-taxi-fare-prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.56G/1.56G [08:15<00:00, 3.38MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive .\\new-york-city-taxi-fare-prediction/new-york-city-taxi-fare-prediction.zip to .\\new-york-city-taxi-fare-prediction\n"
     ]
    }
   ],
   "source": [
    "dataset_url = 'https://www.kaggle.com/c/new-york-city-taxi-fare-prediction'\n",
    "od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xotpf7sjPUXy"
   },
   "outputs": [],
   "source": [
    "# The dataset will be save in a folder called 'new-york-city-taxi-fare-prediction'. We save that folder as data_dir so it will be more handy in case we need to access the data.\n",
    "data_dir = 'new-york-city-taxi-fare-prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRKXBoII6koE"
   },
   "source": [
    "### View Dataset Files\n",
    "\n",
    "Let's look at the size, no. of lines and first few lines of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JSeQME-i6nHb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is DATA"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Volume Serial Number is 284D-6F1A\n",
      "\n",
      " Directory of D:\\Data Projects\\Data_Science_Analysis_Projects\\Machine Learning\\New York Taxi Fare Prediction\n",
      "\n",
      "\n",
      " Directory of D:\\Data Projects\\Data_Science_Analysis_Projects\\Machine Learning\\New York Taxi Fare Prediction\\new-york-city-taxi-fare-prediction\n",
      "\n",
      "14/11/2021  13:03    <DIR>          .\n",
      "14/11/2021  13:03    <DIR>          ..\n",
      "14/11/2021  13:03               486 GCP-Coupons-Instructions.rtf\n",
      "14/11/2021  13:03           343'271 sample_submission.csv\n",
      "14/11/2021  13:03           983'020 test.csv\n",
      "14/11/2021  13:03     5'697'178'298 train.csv\n",
      "               4 File(s)  5'698'505'075 bytes\n",
      "               2 Dir(s)  132'261'031'936 bytes free\n"
     ]
    }
   ],
   "source": [
    "# We can check the content of the folder as follow:\n",
    "!dir -l {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using a whole dataset will make iteration slower. A good practice is that to use a small sample to experiment and iterate faster. Upload the entire dataset in pandas will be slow and also all the afterwards operations will be affected.\n",
    "We will work with a 1% sample of the data (~500k rows ~ hals of the test set size) and at the END come back and work with the entire dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v5HqD9eWP_gE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(55423856, 8)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val = pd.read_csv(data_dir + \"/train.csv\")\n",
    "df_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['key', 'fare_amount', 'pickup_datetime', 'pickup_longitude',\n       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n       'passenger_count'],\n      dtype='object')"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PjwJxDjFQZSM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['key', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count'],\n      dtype='object')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The test data look similar to train data except for the **fair amount** that is the **target column** since we must to predict it. For submitting the preditcion to Kaggle we need a submission file which looks like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "centXUzKQ9op"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                           key  fare_amount\n0  2015-01-27 13:08:24.0000002        11.35\n1  2015-01-27 13:08:24.0000003        11.35\n2  2011-10-08 11:53:44.0000002        11.35\n3  2012-12-01 21:12:12.0000002        11.35\n4  2012-12-01 21:12:12.0000003        11.35",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>fare_amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-27 13:08:24.0000002</td>\n      <td>11.35</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-01-27 13:08:24.0000003</td>\n      <td>11.35</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-10-08 11:53:44.0000002</td>\n      <td>11.35</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-12-01 21:12:12.0000002</td>\n      <td>11.35</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2012-12-01 21:12:12.0000003</td>\n      <td>11.35</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_sub = pd.read_csv(data_dir + \"/sample_submission.csv\")\n",
    "df_sample_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You notice that the key values correspond exactly to the test set. The fare_amont corresponds to the predicted value of out model. In this case the values are 11.35 only because is a sample file. Once you test the model and filled this the submission file you can submit the file to the kaggle competition on the late section [here](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/submit) for check you model score. Feel free to compare your score with the leader board to sse how much your model can be optimized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZD2m2fpRQUq1"
   },
   "source": [
    "Observations:\n",
    "\n",
    "- This is a supervised learning regression problem\n",
    "- Training data is 5.5 GB in size\n",
    "- Training data has 5.5 million rows\n",
    "- Test set is much smaller (< 10,000 rows)\n",
    "- The training set has 8 columns:\n",
    "    - `key` (a unique identifier)\n",
    "    - `fare_amount` (target column)\n",
    "    - `pickup_datetime`\n",
    "    - `pickup_longitude`\n",
    "    - `pickup_latitude`\n",
    "    - `dropoff_longitude`\n",
    "    - `dropoff_latitude`\n",
    "    - `passenger_count`\n",
    "- The test set has all columns except the target column `fare_amount`.\n",
    "- The submission file should contain the `key` and `fare_amount` for each test sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3ECfaXG6opv"
   },
   "source": [
    "### Loading Training Set\n",
    "\n",
    "> _**TIP #2**: When working with large datasets, always start with a sample to experiment & iterate faster._\n",
    "\n",
    "Loading the entire dataset into Pandas is going to be slow, so we can use the following optimizations:\n",
    "\n",
    "- Ignore the `key` column\n",
    "- Parse pickup datetime while loading data \n",
    "- Specify data types for other columns\n",
    "   - `float32` for geo coordinates\n",
    "   - `float32` for fare amount\n",
    "   - `uint8` for passenger count\n",
    "- Work with a 1% sample of the data (~500k rows)\n",
    "\n",
    "We can apply these optimizations while using [`pd.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "BvqQSC5NTK3w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['fare_amount',\n 'pickup_datetime',\n 'pickup_longitude',\n 'pickup_latitude',\n 'dropoff_longitude',\n 'dropoff_latitude',\n 'passenger_count']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_cols = 'fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count'.split(',')\n",
    "sel_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "53-fOkat6qoX"
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'fare_amount' : 'float32',\n",
    "    'pickup_longitude' : 'float32',\n",
    "    'pickup_latitude' : 'float32',\n",
    "    'dropoff_longitude' : 'float32',\n",
    "    'dropoff_latitude' : 'float32',\n",
    "    'passenger_count' : 'uint8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def skiprow(row_idx):\n",
    "    \"\"\"\n",
    "    This function return a True value with a probability defined by the sample_fraction.\n",
    "    The first row will always took.\n",
    "    Input arg: row_idx\n",
    "    Output arg: Boolean\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if row_idx == 0: # Keep the first row\n",
    "        return False\n",
    "    return random.random() > sample_fraction # If random is greater than samplefration (which is 99% of probability it will skip the row"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "elapsed = {\n",
    "    \"1%\" : 0,\n",
    "    \"3%\" : 0,\n",
    "    \"10%\": 0,\n",
    "    \"30%\": 0,\n",
    "    \"100%\": 0\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 1% of the data\n",
    "t = time.time()\n",
    "sample_fraction = 0.01\n",
    "\n",
    "# df_train_val = pd.read_csv(data_dir + \"/train.csv\",usecols = sel_cols, dtype = dtypes, nrows = 500)\n",
    "df_train_val = pd.read_csv(data_dir + \"/train.csv\",\n",
    "                       usecols = sel_cols,\n",
    "                       parse_dates = ['pickup_datetime'], # the types datetime need to pass separately\n",
    "                       dtype = dtypes,\n",
    "                       skiprows = skiprow)\n",
    "elapsed[\"1%\"] = round((time.time() - t)/60,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> _**TIP #3**: Fix the seeds for random number generators so that you get the same results every time you run your notebook._"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Exercise**: Try loading 3%, 10%, 30% and 100% of the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Ye0DzTlkTSV7"
   },
   "outputs": [],
   "source": [
    "# 3% of the data\n",
    "t = time.time()\n",
    "sample_fraction = 0.03\n",
    "\n",
    "# df_train_val = pd.read_csv(data_dir + \"/train.csv\",usecols = sel_cols, dtype = dtypes, nrows = 500)\n",
    "df_train_val_time = pd.read_csv(data_dir + \"/train.csv\",\n",
    "                            usecols = sel_cols,\n",
    "                            parse_dates = ['pickup_datetime'],\n",
    "                            dtype = dtypes,\n",
    "                            skiprows = skiprow)\n",
    "elapsed[\"3%\"] = round((time.time() - t)/60,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "{'1%': 2.39, '3%': 4.99, '10%': 13.83, '30%': 0, '100%': 0}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10% of the data\n",
    "t = time.time()\n",
    "\n",
    "sample_fraction = 0.1\n",
    "\n",
    "# df_train_val = pd.read_csv(data_dir + \"/train.csv\",usecols = sel_cols, dtype = dtypes, nrows = 500)\n",
    "df_train_val_time = pd.read_csv(data_dir + \"/train.csv\",\n",
    "                            usecols = sel_cols,\n",
    "                            parse_dates = ['pickup_datetime'],\n",
    "                            dtype = dtypes,\n",
    "                            skiprows = skiprow)\n",
    "\n",
    "elapsed[\"10%\"] = round((time.time() - t)/60,2)\n",
    "elapsed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "{'1%': 2.39, '3%': 4.99, '10%': 13.83, '30%': 38.3, '100%': 0}"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30% of the data\n",
    "t = time.time()\n",
    "\n",
    "sample_fraction = 0.3\n",
    "\n",
    "# df_train_val = pd.read_csv(data_dir + \"/train.csv\",usecols = sel_cols, dtype = dtypes, nrows = 500)\n",
    "df_train_val_time = pd.read_csv(data_dir + \"/train.csv\",\n",
    "                            usecols = sel_cols,\n",
    "                            parse_dates = ['pickup_datetime'],\n",
    "                            dtype = dtypes,\n",
    "                            skiprows = skiprow)\n",
    "\n",
    "elapsed[\"30%\"] = round((time.time() - t)/60,2)\n",
    "elapsed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# 100% of the data\n",
    "t = time.time()\n",
    "\n",
    "sample_fraction = 1\n",
    "\n",
    "# df_train_val = pd.read_csv(data_dir + \"/train.csv\",usecols = sel_cols, dtype = dtypes, nrows = 500)\n",
    "df_train_val_time = pd.read_csv(data_dir + \"/train.csv\",\n",
    "                            usecols = sel_cols,\n",
    "                            parse_dates = ['pickup_datetime'],\n",
    "                            dtype = dtypes,\n",
    "                            skiprows = skiprow)\n",
    "\n",
    "elapsed[\"100%\"] = round((time.time() - t)/60,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "del df_train_val_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "{'1%': 2.39, '3%': 4.99, '10%': 13.83, '30%': 38.3, '100%': 124.74}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riKehMfZ6vbT"
   },
   "source": [
    "### Load Test Set\n",
    "\n",
    "For the test set, we'll simply provide the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "vOO33o4o6xDS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(9914, 7)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(data_dir + \"/test.csv\",dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "9oqXKAsbTYPl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              key          pickup_datetime  pickup_longitude  \\\n0     2015-01-27 13:08:24.0000002  2015-01-27 13:08:24 UTC        -73.973320   \n1     2015-01-27 13:08:24.0000003  2015-01-27 13:08:24 UTC        -73.986862   \n2     2011-10-08 11:53:44.0000002  2011-10-08 11:53:44 UTC        -73.982521   \n3     2012-12-01 21:12:12.0000002  2012-12-01 21:12:12 UTC        -73.981163   \n4     2012-12-01 21:12:12.0000003  2012-12-01 21:12:12 UTC        -73.966049   \n...                           ...                      ...               ...   \n9909  2015-05-10 12:37:51.0000002  2015-05-10 12:37:51 UTC        -73.968124   \n9910  2015-01-12 17:05:51.0000001  2015-01-12 17:05:51 UTC        -73.945511   \n9911  2015-04-19 20:44:15.0000001  2015-04-19 20:44:15 UTC        -73.991600   \n9912  2015-01-31 01:05:19.0000005  2015-01-31 01:05:19 UTC        -73.985573   \n9913  2015-01-18 14:06:23.0000006  2015-01-18 14:06:23 UTC        -73.988022   \n\n      pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n0           40.763805         -73.981430         40.743835                1  \n1           40.719383         -73.998886         40.739201                1  \n2           40.751259         -73.979652         40.746140                1  \n3           40.767807         -73.990448         40.751637                1  \n4           40.789776         -73.988564         40.744427                1  \n...               ...                ...               ...              ...  \n9909        40.796997         -73.955643         40.780388                6  \n9910        40.803600         -73.960213         40.776371                6  \n9911        40.726608         -73.789742         40.647011                6  \n9912        40.735432         -73.939178         40.801731                6  \n9913        40.754070         -74.000282         40.759220                6  \n\n[9914 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>key</th>\n      <th>pickup_datetime</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-27 13:08:24.0000002</td>\n      <td>2015-01-27 13:08:24 UTC</td>\n      <td>-73.973320</td>\n      <td>40.763805</td>\n      <td>-73.981430</td>\n      <td>40.743835</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-01-27 13:08:24.0000003</td>\n      <td>2015-01-27 13:08:24 UTC</td>\n      <td>-73.986862</td>\n      <td>40.719383</td>\n      <td>-73.998886</td>\n      <td>40.739201</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-10-08 11:53:44.0000002</td>\n      <td>2011-10-08 11:53:44 UTC</td>\n      <td>-73.982521</td>\n      <td>40.751259</td>\n      <td>-73.979652</td>\n      <td>40.746140</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-12-01 21:12:12.0000002</td>\n      <td>2012-12-01 21:12:12 UTC</td>\n      <td>-73.981163</td>\n      <td>40.767807</td>\n      <td>-73.990448</td>\n      <td>40.751637</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2012-12-01 21:12:12.0000003</td>\n      <td>2012-12-01 21:12:12 UTC</td>\n      <td>-73.966049</td>\n      <td>40.789776</td>\n      <td>-73.988564</td>\n      <td>40.744427</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9909</th>\n      <td>2015-05-10 12:37:51.0000002</td>\n      <td>2015-05-10 12:37:51 UTC</td>\n      <td>-73.968124</td>\n      <td>40.796997</td>\n      <td>-73.955643</td>\n      <td>40.780388</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9910</th>\n      <td>2015-01-12 17:05:51.0000001</td>\n      <td>2015-01-12 17:05:51 UTC</td>\n      <td>-73.945511</td>\n      <td>40.803600</td>\n      <td>-73.960213</td>\n      <td>40.776371</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9911</th>\n      <td>2015-04-19 20:44:15.0000001</td>\n      <td>2015-04-19 20:44:15 UTC</td>\n      <td>-73.991600</td>\n      <td>40.726608</td>\n      <td>-73.789742</td>\n      <td>40.647011</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9912</th>\n      <td>2015-01-31 01:05:19.0000005</td>\n      <td>2015-01-31 01:05:19 UTC</td>\n      <td>-73.985573</td>\n      <td>40.735432</td>\n      <td>-73.939178</td>\n      <td>40.801731</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9913</th>\n      <td>2015-01-18 14:06:23.0000006</td>\n      <td>2015-01-18 14:06:23 UTC</td>\n      <td>-73.988022</td>\n      <td>40.754070</td>\n      <td>-74.000282</td>\n      <td>40.759220</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>9914 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "        fare_amount           pickup_datetime  pickup_longitude  \\\n0               4.0 2014-12-06 20:36:22+00:00        -73.979813   \n1               8.0 2013-01-17 17:22:00+00:00          0.000000   \n2               8.9 2011-06-15 18:07:00+00:00        -73.996330   \n3               6.9 2009-12-14 12:33:00+00:00        -73.982430   \n4               7.0 2013-11-06 11:26:54+00:00        -73.959061   \n...             ...                       ...               ...   \n552445         45.0 2014-02-06 23:59:45+00:00        -73.973587   \n552446         22.5 2015-01-05 15:29:08+00:00        -73.935928   \n552447          4.5 2013-02-17 22:27:00+00:00        -73.992531   \n552448         14.5 2013-01-27 12:41:00+00:00        -74.012115   \n552449          6.0 2014-10-18 07:51:00+00:00        -73.997681   \n\n        pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n0             40.751904         -73.979446         40.755482                1  \n1              0.000000           0.000000          0.000000                2  \n2             40.753223         -73.978897         40.766964                3  \n3             40.745747         -73.982430         40.745747                1  \n4             40.781059         -73.962059         40.768604                1  \n...                 ...                ...               ...              ...  \n552445        40.747669         -73.999916         40.602894                1  \n552446        40.799656         -73.985710         40.726952                2  \n552447        40.748619         -73.998436         40.740143                1  \n552448        40.706635         -73.988724         40.756218                1  \n552449        40.724380         -73.994148         40.717796                1  \n\n[552450 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fare_amount</th>\n      <th>pickup_datetime</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.0</td>\n      <td>2014-12-06 20:36:22+00:00</td>\n      <td>-73.979813</td>\n      <td>40.751904</td>\n      <td>-73.979446</td>\n      <td>40.755482</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.0</td>\n      <td>2013-01-17 17:22:00+00:00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.9</td>\n      <td>2011-06-15 18:07:00+00:00</td>\n      <td>-73.996330</td>\n      <td>40.753223</td>\n      <td>-73.978897</td>\n      <td>40.766964</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6.9</td>\n      <td>2009-12-14 12:33:00+00:00</td>\n      <td>-73.982430</td>\n      <td>40.745747</td>\n      <td>-73.982430</td>\n      <td>40.745747</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.0</td>\n      <td>2013-11-06 11:26:54+00:00</td>\n      <td>-73.959061</td>\n      <td>40.781059</td>\n      <td>-73.962059</td>\n      <td>40.768604</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>552445</th>\n      <td>45.0</td>\n      <td>2014-02-06 23:59:45+00:00</td>\n      <td>-73.973587</td>\n      <td>40.747669</td>\n      <td>-73.999916</td>\n      <td>40.602894</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>552446</th>\n      <td>22.5</td>\n      <td>2015-01-05 15:29:08+00:00</td>\n      <td>-73.935928</td>\n      <td>40.799656</td>\n      <td>-73.985710</td>\n      <td>40.726952</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>552447</th>\n      <td>4.5</td>\n      <td>2013-02-17 22:27:00+00:00</td>\n      <td>-73.992531</td>\n      <td>40.748619</td>\n      <td>-73.998436</td>\n      <td>40.740143</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>552448</th>\n      <td>14.5</td>\n      <td>2013-01-27 12:41:00+00:00</td>\n      <td>-74.012115</td>\n      <td>40.706635</td>\n      <td>-73.988724</td>\n      <td>40.756218</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>552449</th>\n      <td>6.0</td>\n      <td>2014-10-18 07:51:00+00:00</td>\n      <td>-73.997681</td>\n      <td>40.724380</td>\n      <td>-73.994148</td>\n      <td>40.717796</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>552450 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9CYWu1c6yD9"
   },
   "source": [
    "## 2. Explore the Dataset\n",
    "\n",
    "- Basic info about training set\n",
    "- Basic info about test set\n",
    "- Exploratory data analysis & visualization\n",
    "- Ask & answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK044Ke76_QB"
   },
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ThJJwhCR7Gzb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 552450 entries, 0 to 552449\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype              \n",
      "---  ------             --------------   -----              \n",
      " 0   fare_amount        552450 non-null  float32            \n",
      " 1   pickup_datetime    552450 non-null  datetime64[ns, UTC]\n",
      " 2   pickup_longitude   552450 non-null  float32            \n",
      " 3   pickup_latitude    552450 non-null  float32            \n",
      " 4   dropoff_longitude  552450 non-null  float32            \n",
      " 5   dropoff_latitude   552450 non-null  float32            \n",
      " 6   passenger_count    552450 non-null  uint8              \n",
      "dtypes: datetime64[ns, UTC](1), float32(5), uint8(1)\n",
      "memory usage: 15.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_train_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "GrcLO35KTsWi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\ncount  552450.000000     552450.000000    552450.000000      552450.000000   \nmean       11.354463        -72.288383        39.830513         -72.295395   \nstd         9.810809         11.622035         8.041162          12.065184   \nmin       -52.000000      -1183.362793     -3084.490234       -3356.729736   \n25%         6.000000        -73.992020        40.734875         -73.991425   \n50%         8.500000        -73.981819        40.752621         -73.980179   \n75%        12.500000        -73.967155        40.767036         -73.963737   \nmax       499.000000       2420.209473       404.983337        2467.752686   \n\n       dropoff_latitude  passenger_count  \ncount     552450.000000    552450.000000  \nmean          39.854305         1.684983  \nstd            9.226158         1.337664  \nmin        -2073.150635         0.000000  \n25%           40.733990         1.000000  \n50%           40.753101         1.000000  \n75%           40.768059         2.000000  \nmax         3351.403076       208.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fare_amount</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>552450.000000</td>\n      <td>552450.000000</td>\n      <td>552450.000000</td>\n      <td>552450.000000</td>\n      <td>552450.000000</td>\n      <td>552450.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>11.354463</td>\n      <td>-72.288383</td>\n      <td>39.830513</td>\n      <td>-72.295395</td>\n      <td>39.854305</td>\n      <td>1.684983</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9.810809</td>\n      <td>11.622035</td>\n      <td>8.041162</td>\n      <td>12.065184</td>\n      <td>9.226158</td>\n      <td>1.337664</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-52.000000</td>\n      <td>-1183.362793</td>\n      <td>-3084.490234</td>\n      <td>-3356.729736</td>\n      <td>-2073.150635</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.000000</td>\n      <td>-73.992020</td>\n      <td>40.734875</td>\n      <td>-73.991425</td>\n      <td>40.733990</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>8.500000</td>\n      <td>-73.981819</td>\n      <td>40.752621</td>\n      <td>-73.980179</td>\n      <td>40.753101</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12.500000</td>\n      <td>-73.967155</td>\n      <td>40.767036</td>\n      <td>-73.963737</td>\n      <td>40.768059</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>499.000000</td>\n      <td>2420.209473</td>\n      <td>404.983337</td>\n      <td>2467.752686</td>\n      <td>3351.403076</td>\n      <td>208.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5jzYKMNTuQc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see that half of the fre is costing less than 8 dollar. That's give us an idea how good our model need to be. An ideal variance of the prediction can be 3 dollars, more than this will be too much. There are some value dirty as negative fare, pickup longitude as -1183 which is not valid (a lot out of the area), and a max passenger of 208."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "(Timestamp('2009-01-01 00:11:46+0000', tz='UTC'),\n Timestamp('2015-06-30 23:59:54+0000', tz='UTC'))"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val[\"pickup_datetime\"].min(),df_train_val[\"pickup_datetime\"].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WIzKrOwTzGv"
   },
   "source": [
    "Observations about training data:\n",
    "\n",
    "- 550k+ rows, as expected\n",
    "- No missing data (in the sample)\n",
    "- `fare_amount` ranges from \\$-52.0 to \\$499.0 \n",
    "- `passenger_count` ranges from 0 to 208 \n",
    "- There seem to be some errors in the latitude & longitude values\n",
    "- Dates range from 1st Jan 2009 to 30th June 2015\n",
    "- The dataset takes up ~19 MB of space in the RAM\n",
    "\n",
    "We may need to deal with outliers and data entry errors before we train our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtRcylCW7Hvu"
   },
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "zK-AtxR_7I6X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9914 entries, 0 to 9913\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   key                9914 non-null   object \n",
      " 1   pickup_datetime    9914 non-null   object \n",
      " 2   pickup_longitude   9914 non-null   float32\n",
      " 3   pickup_latitude    9914 non-null   float32\n",
      " 4   dropoff_longitude  9914 non-null   float32\n",
      " 5   dropoff_latitude   9914 non-null   float32\n",
      " 6   passenger_count    9914 non-null   uint8  \n",
      "dtypes: float32(4), object(2), uint8(1)\n",
      "memory usage: 319.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "lKA33FctWGEe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\ncount       9914.000000      9914.000000        9914.000000       9914.000000   \nmean         -73.976181        40.750954         -73.974945         40.751553   \nstd            0.042799         0.033542           0.039093          0.035436   \nmin          -74.252190        40.573143         -74.263245         40.568974   \n25%          -73.992500        40.736125         -73.991249         40.735253   \n50%          -73.982327        40.753052         -73.980015         40.754065   \n75%          -73.968012        40.767113         -73.964062         40.768757   \nmax          -72.986534        41.709557         -72.990967         41.696682   \n\n       passenger_count  \ncount      9914.000000  \nmean          1.671273  \nstd           1.278747  \nmin           1.000000  \n25%           1.000000  \n50%           1.000000  \n75%           2.000000  \nmax           6.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>9914.000000</td>\n      <td>9914.000000</td>\n      <td>9914.000000</td>\n      <td>9914.000000</td>\n      <td>9914.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-73.976181</td>\n      <td>40.750954</td>\n      <td>-73.974945</td>\n      <td>40.751553</td>\n      <td>1.671273</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.042799</td>\n      <td>0.033542</td>\n      <td>0.039093</td>\n      <td>0.035436</td>\n      <td>1.278747</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-74.252190</td>\n      <td>40.573143</td>\n      <td>-74.263245</td>\n      <td>40.568974</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-73.992500</td>\n      <td>40.736125</td>\n      <td>-73.991249</td>\n      <td>40.735253</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-73.982327</td>\n      <td>40.753052</td>\n      <td>-73.980015</td>\n      <td>40.754065</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-73.968012</td>\n      <td>40.767113</td>\n      <td>-73.964062</td>\n      <td>40.768757</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-72.986534</td>\n      <td>41.709557</td>\n      <td>-72.990967</td>\n      <td>41.696682</td>\n      <td>6.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we have a huge amount of data we can drop the data from the training set which fall out the test set data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "hRudxeE2Wfhm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('2009-01-01 11:04:24 UTC', '2015-06-30 20:03:50 UTC')"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"pickup_datetime\"].min(),df_test[\"pickup_datetime\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQRCB4HiWLYb"
   },
   "source": [
    "Some observations about the test set:\n",
    "\n",
    "- 9914 rows of data\n",
    "- No missing values\n",
    "- No obvious data entry errors\n",
    "- 1 to 6 passengers (we can limit training data to this range)\n",
    "- Latitudes lie between 40 and 42\n",
    "- Longitudes lie between -75 and -72\n",
    "- Pickup dates range from Jan 1st 2009 to Jun  30th 2015 (same as training set)\n",
    "\n",
    "We can use the ranges of the test set to drop outliers/invalid data from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckZBVEMt7PMa"
   },
   "source": [
    "### Exploratory Data Analysis and Visualization\n",
    "\n",
    "**Exercise**: Create graphs (histograms, line charts, bar charts, scatter plots, box plots, geo maps etc.) to study the distrubtion of values in each column, and the relationship of each input column to the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdYzucgt7RkC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoQzsx04W-oC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-NiN3shYpbj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpkjXgOr7Yaq"
   },
   "source": [
    "### Ask & Answer Questions\n",
    "\n",
    "**Exercise**: Ask & answer questions about the dataset: \n",
    "\n",
    "1. What is the busiest day of the week?\n",
    "2. What is the busiest time of the day?\n",
    "3. In which month are fares the highest?\n",
    "4. Which pickup locations have the highest fares?\n",
    "5. Which drop locations have the highest fares?\n",
    "6. What is the average ride distance?\n",
    "7. ???\n",
    "\n",
    "Performing EDA on your dataset and asking questions will help you develop a deeper understand of the data and give you ideas for feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE0-j7ibYtdt"
   },
   "source": [
    "\n",
    "Resources for exploratory analysis & visualization:\n",
    "\n",
    "- EDA project from scratch: [https://www.youtube.com/watch?v=kLDTbavcmd0](https://www.youtube.com/watch?v=kLDTbavcmd0)\n",
    "- Data Analysis with Python: [https://zerotopandas.com](https://zerotopandas.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBoP2YdZZOcL"
   },
   "source": [
    "> _**TIP #4**: Take an iterative approach to building ML models: do some EDA, do some feature engineering, train a model, then repeat to improve your model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo0ue49U7eu4"
   },
   "source": [
    "## 3. Prepare Dataset for Training\n",
    "\n",
    "- Split Training & Validation Set\n",
    "- Fill/Remove Missing Values\n",
    "- Extract Inputs & Outputs\n",
    "   - Training\n",
    "   - Validation\n",
    "   - Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mswEXeiu7mrA"
   },
   "source": [
    "### Split Training & Validation Set\n",
    "\n",
    "We'll set aside 20% of the training data as the validation set, to evaluate the models we train on previously unseen data. \n",
    "\n",
    "Since the test set and training set have the same date ranges, we can pick a random 20% fraction.\n",
    "\n",
    "> _**TIP #5**: Your validation set should be as similar to the test set or real-world data as possible i.e. the evaluation metric score of a model on validation & test sets should be very close, otherwise you're shooting in the dark._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try also with StratifiedShuffleSplit method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "-A1hKczB7U0I"
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_train_val, test_size=0.2, random_state=42) # We set random_state=42 so we always have the same test and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "223dkJPcZ5wM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(441960, 110490)"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fci2Mg9mZ6Dj",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# StShSp = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#\n",
    "# for train_idx, valid_idx in StShSp.split(df_train_val, df_train_val[\"fare_amount\"]):\n",
    "#     train_data = train_df.loc[train_idx]\n",
    "#     valid_data = train_df.loc[valid_ids]\n",
    "# train_data['price_category'].value_counts() / len(train_data), test_data['price_category'].value_counts()/len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RREUhZFx7k37"
   },
   "source": [
    "### Fill/Remove Missing Values\n",
    "\n",
    "There are no missing values in our sample, but if there were, we could simply drop the rows with missing values instead of trying to fill them (since we have a lot of training data)> Since the missinga value is estimated around 1% or 2% and we have many data we can drop this values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "62tQ1Mkj-B1U"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8aL-x9C-Gc5"
   },
   "source": [
    "### Extract Inputs and Outputs\n",
    "Before create a model we need to separate the input and the outputs columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "3EznRfztapJd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count'],\n      dtype='object')"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can't pass a datetime colum into a ML model by himself because it's a timestamp not a number. We need to convert that column into sum or split it into multiple columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "20_-9TgMapjR"
   },
   "outputs": [],
   "source": [
    "input_cols = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "7CAarE7UapzF"
   },
   "outputs": [],
   "source": [
    "target_cols = 'fare_amount'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b-bhEhS-Mz6"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "1zTNkuqe-H3O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        pickup_longitude  pickup_latitude  dropoff_longitude  \\\n353352        -73.993652        40.741543         -73.977974   \n360070        -73.993805        40.724579         -73.993805   \n372609        -73.959160        40.780750         -73.969116   \n550895        -73.952187        40.783951         -73.978645   \n444151        -73.977112        40.746834         -73.991104   \n...                  ...              ...                ...   \n110268        -73.987152        40.750633         -73.979073   \n259178        -73.972656        40.764042         -74.013176   \n365838        -73.991982        40.749767         -73.989845   \n131932        -73.969055        40.761398         -73.990814   \n121958        -73.983040        40.742142         -74.002510   \n\n        dropoff_latitude  passenger_count  \n353352         40.742352                4  \n360070         40.724579                1  \n372609         40.761230                1  \n550895         40.772602                1  \n444151         40.750404                2  \n...                  ...              ...  \n110268         40.763168                1  \n259178         40.707840                2  \n365838         40.720551                3  \n131932         40.751328                1  \n121958         40.727650                1  \n\n[441960 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>353352</th>\n      <td>-73.993652</td>\n      <td>40.741543</td>\n      <td>-73.977974</td>\n      <td>40.742352</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>360070</th>\n      <td>-73.993805</td>\n      <td>40.724579</td>\n      <td>-73.993805</td>\n      <td>40.724579</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>372609</th>\n      <td>-73.959160</td>\n      <td>40.780750</td>\n      <td>-73.969116</td>\n      <td>40.761230</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>550895</th>\n      <td>-73.952187</td>\n      <td>40.783951</td>\n      <td>-73.978645</td>\n      <td>40.772602</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>444151</th>\n      <td>-73.977112</td>\n      <td>40.746834</td>\n      <td>-73.991104</td>\n      <td>40.750404</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110268</th>\n      <td>-73.987152</td>\n      <td>40.750633</td>\n      <td>-73.979073</td>\n      <td>40.763168</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>259178</th>\n      <td>-73.972656</td>\n      <td>40.764042</td>\n      <td>-74.013176</td>\n      <td>40.707840</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>365838</th>\n      <td>-73.991982</td>\n      <td>40.749767</td>\n      <td>-73.989845</td>\n      <td>40.720551</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>131932</th>\n      <td>-73.969055</td>\n      <td>40.761398</td>\n      <td>-73.990814</td>\n      <td>40.751328</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>121958</th>\n      <td>-73.983040</td>\n      <td>40.742142</td>\n      <td>-74.002510</td>\n      <td>40.727650</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>441960 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs = train_df[input_cols]\n",
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "rsdfMMSybTsQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "353352     6.0\n360070     3.7\n372609    10.0\n550895     8.9\n444151     7.3\n          ... \n110268     9.3\n259178    18.5\n365838    10.1\n131932    10.9\n121958     9.5\nName: fare_amount, Length: 441960, dtype: float32"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_inputs = train_df[target_cols]\n",
    "target_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj49BUY_-InI"
   },
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "FR-5yVjY-PZV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        pickup_longitude  pickup_latitude  dropoff_longitude  \\\n15971         -73.995834        40.759190         -73.973679   \n149839        -73.977386        40.738335         -73.976143   \n515867        -73.983910        40.749470         -73.787170   \n90307         -73.790794        40.643463         -73.972252   \n287032        -73.976593        40.761944         -73.991463   \n...                  ...              ...                ...   \n467556        -73.968567        40.761238         -73.983406   \n19482         -73.986725        40.755920         -73.985855   \n186063          0.000000         0.000000           0.000000   \n382260        -73.980057        40.760334         -73.872589   \n18838         -73.955406        40.782417         -73.960434   \n\n        dropoff_latitude  passenger_count  \n15971          40.739086                1  \n149839         40.751205                1  \n515867         40.646645                1  \n90307          40.690182                1  \n287032         40.750309                2  \n...                  ...              ...  \n467556         40.750019                3  \n19482          40.731171                1  \n186063          0.000000                1  \n382260         40.774300                1  \n18838          40.767666                1  \n\n[110490 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15971</th>\n      <td>-73.995834</td>\n      <td>40.759190</td>\n      <td>-73.973679</td>\n      <td>40.739086</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149839</th>\n      <td>-73.977386</td>\n      <td>40.738335</td>\n      <td>-73.976143</td>\n      <td>40.751205</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>515867</th>\n      <td>-73.983910</td>\n      <td>40.749470</td>\n      <td>-73.787170</td>\n      <td>40.646645</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>90307</th>\n      <td>-73.790794</td>\n      <td>40.643463</td>\n      <td>-73.972252</td>\n      <td>40.690182</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>287032</th>\n      <td>-73.976593</td>\n      <td>40.761944</td>\n      <td>-73.991463</td>\n      <td>40.750309</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>467556</th>\n      <td>-73.968567</td>\n      <td>40.761238</td>\n      <td>-73.983406</td>\n      <td>40.750019</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>19482</th>\n      <td>-73.986725</td>\n      <td>40.755920</td>\n      <td>-73.985855</td>\n      <td>40.731171</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>186063</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>382260</th>\n      <td>-73.980057</td>\n      <td>40.760334</td>\n      <td>-73.872589</td>\n      <td>40.774300</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18838</th>\n      <td>-73.955406</td>\n      <td>40.782417</td>\n      <td>-73.960434</td>\n      <td>40.767666</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>110490 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs = val_df[input_cols]\n",
    "val_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "bL75iGCkbe3t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "15971     14.000000\n149839     6.500000\n515867    49.570000\n90307     49.700001\n287032     8.500000\n            ...    \n467556     6.100000\n19482      7.300000\n186063     4.500000\n382260    32.900002\n18838     11.500000\nName: fare_amount, Length: 110490, dtype: float32"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targets = val_df[target_cols]\n",
    "val_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxUKnWZG-SEP"
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "eszwMU1U-S04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n0           -73.973320        40.763805         -73.981430         40.743835   \n1           -73.986862        40.719383         -73.998886         40.739201   \n2           -73.982521        40.751259         -73.979652         40.746140   \n3           -73.981163        40.767807         -73.990448         40.751637   \n4           -73.966049        40.789776         -73.988564         40.744427   \n...                ...              ...                ...               ...   \n9909        -73.968124        40.796997         -73.955643         40.780388   \n9910        -73.945511        40.803600         -73.960213         40.776371   \n9911        -73.991600        40.726608         -73.789742         40.647011   \n9912        -73.985573        40.735432         -73.939178         40.801731   \n9913        -73.988022        40.754070         -74.000282         40.759220   \n\n      passenger_count  \n0                   1  \n1                   1  \n2                   1  \n3                   1  \n4                   1  \n...               ...  \n9909                6  \n9910                6  \n9911                6  \n9912                6  \n9913                6  \n\n[9914 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n      <th>passenger_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-73.973320</td>\n      <td>40.763805</td>\n      <td>-73.981430</td>\n      <td>40.743835</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-73.986862</td>\n      <td>40.719383</td>\n      <td>-73.998886</td>\n      <td>40.739201</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-73.982521</td>\n      <td>40.751259</td>\n      <td>-73.979652</td>\n      <td>40.746140</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-73.981163</td>\n      <td>40.767807</td>\n      <td>-73.990448</td>\n      <td>40.751637</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-73.966049</td>\n      <td>40.789776</td>\n      <td>-73.988564</td>\n      <td>40.744427</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9909</th>\n      <td>-73.968124</td>\n      <td>40.796997</td>\n      <td>-73.955643</td>\n      <td>40.780388</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9910</th>\n      <td>-73.945511</td>\n      <td>40.803600</td>\n      <td>-73.960213</td>\n      <td>40.776371</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9911</th>\n      <td>-73.991600</td>\n      <td>40.726608</td>\n      <td>-73.789742</td>\n      <td>40.647011</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9912</th>\n      <td>-73.985573</td>\n      <td>40.735432</td>\n      <td>-73.939178</td>\n      <td>40.801731</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9913</th>\n      <td>-73.988022</td>\n      <td>40.754070</td>\n      <td>-74.000282</td>\n      <td>40.759220</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>9914 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs = df_test[input_cols]\n",
    "test_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D69gMpoc-T3x"
   },
   "source": [
    "## 4. Train Hardcoded & Baseline Models\n",
    "\n",
    "> _**TIP #6**: Always create a simple hardcoded or baseline model to establish the minimum score any proper ML model should beat._\n",
    "\n",
    "- Hardcoded model: always predict average fare\n",
    "- Baseline model: Linear regression \n",
    "\n",
    "For evaluation the dataset uses RMSE error: \n",
    "[https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HojOJqvh-8I2"
   },
   "source": [
    "### Train & Evaluate Hardcoded Model\n",
    "\n",
    "Let's create a simple model that always predicts the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egLCJe0TdrGF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi3N8Kkh-_SZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "br8KFvGQdeWW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnR-_BSzdeqO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyJOWf0-dkbr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Mp2LEKHdktj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkbSKOZqdm0z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvJkxaImdnP1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHoIUSyXdupG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kedz-UOFd4kM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJLGGq3Rdu6U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6q6zRkNyeLSm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O46X0AFWdyYx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXVmrtzRd7lb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52_3bDeJeOLl"
   },
   "source": [
    "Our dumb hard-coded model is off by \\$9.899 on average, which is pretty bad considering the average fare is \\$11.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd9QsynHd7yb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwWoP6if-_uA"
   },
   "source": [
    "### Train & Evaluate Baseline Model\n",
    "\n",
    "We'll train a linear regression model as our baseline, which tries to express the target as a weighted sum of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2ivNSaf_Bh3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSsDURPWecmP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9vkP2Wbec3Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFYfI2fledIo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyeI6iZmemFM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Gv3omU8emm-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6SIpnucem5D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3y9WJfSe6Um"
   },
   "source": [
    "The linear regression model is off by $9.898, which isn't much better than simply predicting the average. \n",
    "\n",
    "This is mainly because the training data (geocoordinates) is not in a format that's useful for the model, and we're not using one of the most important columns: pickup date & time.\n",
    "\n",
    "However, now we have a baseline that our other models should ideally beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqVt3xmsgJwz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c1nCUvn_B_o"
   },
   "source": [
    "## 5. Make Predictions and Submit to Kaggle\n",
    "\n",
    "> _**TIP #7**: When working on a Kaggle competition, submit early and submit often (ideally daily). The best way to improve your models is to try & beat your previous score._\n",
    "\n",
    "- Make predictions for test set\n",
    "- Generate submissions CSV\n",
    "- Submit to Kaggle\n",
    "- Record in experiment tracking sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtRdQgUV_Fh4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaFg2Pc2gaOU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JatqolD1gado"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UM0edzAgatI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWqY-qJ4bnlf"
   },
   "source": [
    "> _**TIP #8**: Create reusable functions for common tasks. They'll help you iterate faster and free up your mind to think about new ideas._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1rSS5Ctgjs_"
   },
   "outputs": [],
   "source": [
    "def predict_and_submit(model, fname):\n",
    "    test_preds = model.predict(test_inputs)\n",
    "    sub_df = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "    sub_df['fare_amount'] = test_preds\n",
    "    sub_df.to_csv(fname, index=None)\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo7t2RcYgkCL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA3RD2_BjHo7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IBHaekXgkSD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZpNqiYieqJs"
   },
   "source": [
    "> _**TIP #9**: Track your ideas & experiments systematically to avoid become overwhelmed with dozens of models. Use this template: https://bit.ly/mltrackingsheet_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEbYT2mG_HgG"
   },
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "> _**TIP #10**: Take an iterative approach to feature engineering. Add some features, train a model, evaluate it, keep the features if they help, otherwise drop them, then repeat._\n",
    "\n",
    "- Extract parts of date\n",
    "- Remove outliers & invalid data\n",
    "- Add distance between pickup & drop\n",
    "- Add distance from landmarks\n",
    "\n",
    "Exercise: We're going to apply all of the above together, but you should observer the effect of adding each feature individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcUB8lxy_sg6"
   },
   "source": [
    "### Extract Parts of Date\n",
    "\n",
    "- Year\n",
    "- Month\n",
    "- Day\n",
    "- Weekday\n",
    "- Hour\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2CivTql_2Fu"
   },
   "outputs": [],
   "source": [
    "def add_dateparts(df, col):\n",
    "    df[col + '_year'] = df[col].dt.year\n",
    "    df[col + '_month'] = df[col].dt.month\n",
    "    df[col + '_day'] = df[col].dt.day\n",
    "    df[col + '_weekday'] = df[col].dt.weekday\n",
    "    df[col + '_hour'] = df[col].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOx5_kxOjZB5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDXVCaADjZT-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykp-Xdy-jY7A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzvyUWROkhfG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kr1KUnq8khXQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqmAKOdfASXX"
   },
   "source": [
    "### Add Distance Between Pickup and Drop\n",
    "\n",
    "We can use the haversine distance: \n",
    "- https://en.wikipedia.org/wiki/Haversine_formula\n",
    "- https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmFyo4MRCB67"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahz4-U7GzoEM"
   },
   "outputs": [],
   "source": [
    "def add_trip_distance(df):\n",
    "    df['trip_distance'] = haversine_np(df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvqOy7V409FF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojOOKh7i5rhC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gqd69jAX5tXY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMRIiNqw08uZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFpeLO4MCP4J"
   },
   "source": [
    "### Add Distance From Popular Landmarks\n",
    "\n",
    "> _**TIP #11**: Creative feature engineering (generally involving human insight or external data) is a lot more effective than excessive hyperparameter tuning. Just one or two good feature improve the model's performance drastically._\n",
    "\n",
    "- JFK Airport\n",
    "- LGA Airport\n",
    "- EWR Airport\n",
    "- Times Square\n",
    "- Met Meuseum\n",
    "- World Trade Center\n",
    "\n",
    "We'll add the distance from drop location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_hrTiYRAO5D"
   },
   "outputs": [],
   "source": [
    "jfk_lonlat = -73.7781, 40.6413\n",
    "lga_lonlat = -73.8740, 40.7769\n",
    "ewr_lonlat = -74.1745, 40.6895\n",
    "met_lonlat = -73.9632, 40.7794\n",
    "wtc_lonlat = -74.0099, 40.7126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLTcCYNKCS04"
   },
   "outputs": [],
   "source": [
    "def add_landmark_dropoff_distance(df, landmark_name, landmark_lonlat):\n",
    "    lon, lat = landmark_lonlat\n",
    "    df[landmark_name + '_drop_distance'] = haversine_np(lon, lat, df['dropoff_longitude'], df['dropoff_latitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuKlVwkzBNRl"
   },
   "outputs": [],
   "source": [
    "def add_landmarks(df):\n",
    "  landmarks = [('jfk', jfk_lonlat), ('lga', lga_lonlat), ('ewr', ewr_lonlat), ('met', met_lonlat), ('wtc', wtc_lonlat)]\n",
    "  for name, lonlat in landmarks:\n",
    "    add_landmark_dropoff_distance(a_df, name, lonlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vspm4UZlBx0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wh54p18fcbAU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USE10_QUcbY3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm99KRT__3GS"
   },
   "source": [
    "### Remove Outliers and Invalid Data\n",
    "\n",
    "There seems to be some invalid data in each of the following columns:\n",
    "\n",
    "- Fare amount\n",
    "- Passenger count\n",
    "- Pickup latitude & longitude\n",
    "- Drop latitude & longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lu1iN719AQX9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCw4gevKcedV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0Z33hGclw04"
   },
   "source": [
    "We'll use the following ranges:\n",
    "\n",
    "- `fare_amount`: \\$1 to \\$500\n",
    "- `longitudes`: -75 to -72\n",
    "- `latitudes`: 40 to 42\n",
    "- `passenger_count`: 1 to 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HXy4CHsk_sI"
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    return df[(df['fare_amount'] >= 1.) & \n",
    "              (df['fare_amount'] <= 500.) &\n",
    "              (df['pickup_longitude'] >= -75) & \n",
    "              (df['pickup_longitude'] <= -72) & \n",
    "              (df['dropoff_longitude'] >= -75) & \n",
    "              (df['dropoff_longitude'] <= -72) & \n",
    "              (df['pickup_latitude'] >= 40) & \n",
    "              (df['pickup_latitude'] <= 42) & \n",
    "              (df['dropoff_latitude'] >=40) & \n",
    "              (df['dropoff_latitude'] <= 42) & \n",
    "              (df['passenger_count'] >= 1) & \n",
    "              (df['passenger_count'] <= 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQPGJQmik_pR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqndbWx-k_jG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIQjeVA6yACo"
   },
   "source": [
    "### Scaling and One-Hot Encoding\n",
    "\n",
    "**Exercise**: Try scaling numeric columns to the `(0,1)` range and encoding categorical columns using a one-hot encoder.\n",
    "\n",
    "We won't do this because we'll be training tree-based models which are generally able to do a good job even without the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VZfxghqD7Ow"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pgx8tBEyD7im"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFbB6PkkD9X2"
   },
   "source": [
    "### Save Intermediate DataFrames\n",
    "\n",
    "> _**TIP #12**: Save preprocessed & prepared data files to save time & experiment faster. You may also want to create differnt notebooks for EDA, feature engineering and model training._\n",
    "\n",
    "Let's save the processed datasets in the Apache Parquet format, so that we can load them back easily to resume our work from this point.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w26KhYmIEBO0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u7e8FxdEBsR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjAvvW6_EmVI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaILwZZNyeob"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SxgQec8E8iF"
   },
   "source": [
    "## 7. Train & Evaluate Different Models\n",
    "\n",
    "We'll train each of the following & submit predictions to Kaggle:\n",
    "\n",
    "- Linear Regression\n",
    "- Random Forests\n",
    "- Gradient Boosting\n",
    "\n",
    "Exercise: Train Ridge, SVM, KNN, Decision Tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP_jiS_ZFDL2"
   },
   "source": [
    "### Split Inputs & Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-nmx270E-7z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vy778UxeE-eG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgeRhiSdE-Rr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8Js99j5F450"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIMlsS7pF4qd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m47bx3LEF9Iu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-gkmg0BFG4b"
   },
   "source": [
    "Let's define a helper function to evaluate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSnSkTrXExU1"
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    train_preds = model.predict(train_inputs)\n",
    "    train_rmse = mean_squared_error(train_targets, train_preds, squared=False)\n",
    "    val_preds = model.predict(val_inputs)\n",
    "    val_rmse = mean_squared_error(val_targets, val_preds, squared=False)\n",
    "    return train_rmse, val_rmse, train_preds, val_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOtwHcOzFRn4"
   },
   "source": [
    "### Ridge Regression\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHGpiRjbGjFf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nf2a_i3FTV7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_Gk8A4qGtea"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nHsAUg5GtT0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrTd7DSuG4og"
   },
   "source": [
    "Our model was able to get to an RMSE of $5.2, much better than our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leUTkRhVG4T2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asLsXKZxHceo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be_1ibE3G4Fg"
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwLUw-xXFdRj"
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynZuqnImHuR3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYWVBE0bHuOg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEC8Ll3LHuLN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUnb5t8dFe7c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTXYvMefIaPM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nvcXsLnIozx"
   },
   "source": [
    "\n",
    "This puts us at position ~570 out of 1483 i.e. top 40%, which is already a really good score. \n",
    "\n",
    "Remember that we're only using 1% of the data, and we haven't done much hyperparameter tuning yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpM6yKErIoS0"
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK7hXI8IFgZ5"
   },
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "See https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpPz5PvBFjxb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWka7h5_ycx2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0RqiicaInkM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htjm_0kpIn1u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmDlr-AbLIFk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LqXWIBCLxUR"
   },
   "source": [
    "\n",
    "\n",
    "This submission isn't as good as the random forest, but there's scope for improvement with Hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvZ2zv0XLw8r"
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlXJfN46FvnD"
   },
   "source": [
    "## 8. Tune Hyperparmeters\n",
    "\n",
    "https://towardsdatascience.com/mastering-xgboost-2eb6bce6bc76\n",
    "\n",
    "\n",
    "We'll train parameters for the XGBoost model. Here’s a strategy for tuning hyperparameters:\n",
    "\n",
    "- Tune the most important/impactful hyperparameter first e.g. n_estimators\n",
    "\n",
    "- With the best value of the first hyperparameter, tune the next most impactful hyperparameter\n",
    "\n",
    "- And so on, keep training the next most impactful parameters with the best values for previous parameters...\n",
    "\n",
    "- Then, go back to the top and further tune each parameter again for further marginal gains\n",
    "\n",
    "- Hyperparameter tuning is more art than science, unfortunately. Try to get a feel for how the parameters interact with each other based on your understanding of the parameter…\n",
    "\n",
    "Let's define a helper function for trying different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdOzypc7MN-x"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_params(ModelClass, **params):\n",
    "    \"\"\"Trains a model with the given parameters and returns training & validation RMSE\"\"\"\n",
    "    model = ModelClass(**params).fit(train_inputs, train_targets)\n",
    "    train_rmse = mean_squared_error(model.predict(train_inputs), train_targets, squared=False)\n",
    "    val_rmse = mean_squared_error(model.predict(val_inputs), val_targets, squared=False)\n",
    "    return train_rmse, val_rmse\n",
    "\n",
    "def test_param_and_plot(ModelClass, param_name, param_values, **other_params):\n",
    "    \"\"\"Trains multiple models by varying the value of param_name according to param_values\"\"\"\n",
    "    train_errors, val_errors = [], [] \n",
    "    for value in param_values:\n",
    "        params = dict(other_params)\n",
    "        params[param_name] = value\n",
    "        train_rmse, val_rmse = test_params(ModelClass, **params)\n",
    "        train_errors.append(train_rmse)\n",
    "        val_errors.append(val_rmse)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title('Overfitting curve: ' + param_name)\n",
    "    plt.plot(param_values, train_errors, 'b-o')\n",
    "    plt.plot(param_values, val_errors, 'r-o')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGX0-Lm4OGhx"
   },
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'reg:squarederror'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhEXG369HLLA"
   },
   "source": [
    "### No. of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz2QgfIqHOvx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_Qj968PPSMZ"
   },
   "source": [
    "Seems like 500 estimators has the lowest validation loss. However, it also takes a long time. Let's stick with 250 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWDujeerPSo5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV3jbIapHIpp"
   },
   "source": [
    "### Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeF82yZTHKuq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG3dBJkYQjfv"
   },
   "source": [
    "Looks like a max depth of 5 is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwF0TM08QkIY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVEN-IY4HGgJ"
   },
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ok26BhAlFyK_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y414md7SYMo"
   },
   "source": [
    "Seems like the best learning rate is 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvJDejVNSYet"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izR7k6LXHdCQ"
   },
   "source": [
    "### Other Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxVopdbbOEcq"
   },
   "source": [
    "Similarly we can experiment with other parameters. \n",
    "\n",
    "Here's a set of parameters that works well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5cxbbzqNwfk"
   },
   "outputs": [],
   "source": [
    "xgb_model_final = XGBRegressor(objective='reg:squarederror', \n",
    "                               n_jobs=-1, \n",
    "                               random_state=42,\n",
    "                               n_estimators=500, \n",
    "                               max_depth=5, \n",
    "                               learning_rate=0.1, \n",
    "                               subsample=0.8, \n",
    "                               colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0CNujFUSTpz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMskj9MiHmBT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTMSYvczTOMx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KSk5Hr9Thsj"
   },
   "source": [
    "\n",
    "\n",
    "This puts us at the ~460th position out of 1483 i.e. top 30%. This is pretty amazing considering:\n",
    "\n",
    "- We are using just 1% of the training data\n",
    "- We are only using a single model (most top submissions use ensembles)\n",
    "- Our best model takes just 10 minutes to train (as oppposed to hours/days)\n",
    "- We haven't fully optimized the hyperparameters yet\n",
    "\n",
    "Let's save the weights of this model. Follow this guide: https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5hZNwxIThdN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QU_QcincWhsA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWE4S1MBWiRI"
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg52gR1hIH9E"
   },
   "source": [
    "**Exercises**: \n",
    "\n",
    "1. Tune hyperparameters for Linear Regression & random forests.\n",
    "2. Repeat with 3%, 10%, 30% and 100% of the training set. How much reduction in error does 100x more data produce?\n",
    "3. Ensemble (average) the results from multiple models and observe if they're better than individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrlKunTpU5CB"
   },
   "source": [
    "### Save Model Weights to Google Drive (Optional)\n",
    "\n",
    "We can save all the output files we've created to Google Drive, so that we can reuse them later if required.\n",
    "\n",
    "Follow these guides: \n",
    "- https://scikit-learn.org/stable/modules/model_persistence.html\n",
    "- https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IFHEsklVCKb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6T1lsTeVCdT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMA4a4a0WAkE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEgYBB7SHpIe"
   },
   "source": [
    "## 9. Train on GPU with entire dataset (Optional)\n",
    "\n",
    "Steps:\n",
    "- Install `dask`, `cudf` and `cuml`\n",
    "- Load the dataset to GPU\n",
    "- Create training and validation set\n",
    "- Perform feature engineering\n",
    "- Train XGBoost `cuml` model\n",
    "- Make predictions & submit\n",
    "\n",
    "Follow these guides and fill out the empty cells below:\n",
    "- https://towardsdatascience.com/nyc-taxi-fare-prediction-605159aa9c24\n",
    "- https://jovian.ai/allenkong221/nyc-taxi-fare-rapids-dask-gpu/v/1?utm_source=embed#C10\n",
    "- https://developer.nvidia.com/blog/accelerating-xgboost-on-gpu-clusters-with-dask/\n",
    "- https://rapids.ai/xgboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl9xnk85Hsbq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8XY3M4LH8xB"
   },
   "source": [
    "### Install `dask`, `cudf` and `cuml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tK78LbdIf1m"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T0mffaGIgTg"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbcghTdkIhaF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbAgCLouIh0q"
   },
   "source": [
    "### Create training & validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_2j3bfdIjjE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD2FJpDRIkJo"
   },
   "source": [
    "### Perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU7eGJ79Ilrs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5Fauiz4JNQa"
   },
   "source": [
    "### Train XGBoost model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JolgKXPYJO3c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUFO3Id3JPbk"
   },
   "source": [
    "### Make Predictions & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sitr9vvNJROO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2Y3EUAIJSai"
   },
   "source": [
    "## 10. Document & Publish Your Work\n",
    "\n",
    "> _**TIP #13**: Always document & publish your projects online. They help improve your understanding, showcase your skills & often lead to inbound job opportunities._\n",
    "\n",
    "- Add explanations using Markdown\n",
    "- Clean up the code & create functions\n",
    "- Publish notebook to Jovian\n",
    "- Write a blog post and embed\n",
    "\n",
    "Follow this guide: https://www.youtube.com/watch?v=NK6UYg3-Bxs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AA1qCyqVLew"
   },
   "source": [
    "## References\n",
    "\n",
    "* Dataset: https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview\n",
    "* Missing semester (Shell scripting): https://missing.csail.mit.edu/\n",
    "* Opendatsets library: https://github.com/JovianML/opendatasets \n",
    "* EDA project from scratch: https://www.youtube.com/watch?v=kLDTbavcmd0\n",
    "* GeoPy: https://geopy.readthedocs.io/en/stable/#module-geopy.distance \n",
    "* Blog post by Allen Kong: https://towardsdatascience.com/nyc-taxi-fare-prediction-605159aa9c24 \n",
    "* Machine Learning with Python: Zero to GBMs - https://zerotogbms.com \n",
    "* Experiment tracking spreadsheet: https://bit.ly/mltrackingsheet \n",
    "* Pandas datetime components: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-date-components \n",
    "* Haversine distance: https://en.wikipedia.org/wiki/Haversine_formula \n",
    "* Haversine distance with Numpy: https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas \n",
    "* RAPIDS (parent project for cudf and cuml): https://rapids.ai/\n",
    "* Data Science blog post from scratch: https://www.youtube.com/watch?v=NK6UYg3-Bxs \n",
    "* Examples of Machine Learning Projects:\n",
    "    * Walmart Store Sales: https://jovian.ai/anushree-k/final-walmart-simple-rf-gbm\n",
    "    * Used Car Price Prediction: https://jovian.ai/kara-mounir/used-cars-prices \n",
    "    * Lithology Prediction: https://jovian.ai/ramysaleem/ml-project-machine-predicting-lithologies\n",
    "    * Ad Demand Prediction: https://jovian.ai/deepa-sarojam/online-ad-demand-prediction-ml-prj \n",
    "    * Financial distress prediction: https://jovian.ai/sm-wilson/ml-project-financial-distress-prediction\n",
    "    * Credit scoring: https://jovian.ai/shenghongzhong/credit-scores-algorithms-ml-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "executionInfo": {
     "elapsed": 14991,
     "status": "ok",
     "timestamp": 1635602667676,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEhN-SUD4acXtkX_d8m7mxWmrngRSQYIe55adBOg=s64",
      "userId": "05836266386358934739"
     },
     "user_tz": -330
    },
    "id": "lUvguNEdgTbM",
    "outputId": "75e5503e-48d9-42c0-de82-6f16c2498814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001B[0m\n",
      "[jovian] Please enter your API key ( from https://jovian.ai/ ):\u001B[0m\n",
      "API KEY: ··········\n",
      "[jovian] Uploading colab notebook to Jovian...\u001B[0m\n",
      "Committed successfully! https://jovian.ai/aakashns/nyc-taxi-fare-prediction-blank\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/aakashns/nyc-taxi-fare-prediction-blank'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drSwCN9HgUdW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "nyc_taxi_fare_prediction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}